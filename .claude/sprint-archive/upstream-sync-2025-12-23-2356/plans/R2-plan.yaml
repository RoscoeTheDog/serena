# Plan: R2
# Generated by: discovery:R2.d
# Timestamp: 2025-12-23T23:00:00Z

story_id: "R2"
title: "Generate Test Evidence for Story 6"
created: "2025-12-23T23:00:00Z"
created_by: "discovery:R2.d"

# Analysis Summary
analysis:
  summary: "Run full integration test suite (1030 tests) and capture results in JSON format to create missing test-results artifact for Story 6"
  complexity: "medium"
  estimated_files: 1
  estimated_tokens: 5000
  risk_factors:
    - "Test count discrepancy: Story 6 claimed 138 tests but pytest collected 1030 tests"
    - "Some language-specific tests (Java, Rust, Erlang) may fail or be slow due to language server requirements"
    - "Integration tests may require specific MCP server configurations"
    - "Test execution may take 5-15 minutes depending on system resources"

# Files to Create
files_to_create:
  - path: ".claude/sprint/test-results/6-results.json"
    purpose: "Test results artifact with comprehensive test execution data"
    pattern_source: "Based on AC-R2.2 schema specification in R2 story file"
    estimated_lines: 500-2000

# Files to Modify
files_to_modify: []

# Integration Points
integration:
  - file: ".claude/sprint/test-results/"
    type: "directory"
    description: "Create test-results directory if it doesn't exist"

# Patterns to Follow
patterns:
  - source: "pyproject.toml [tool.poe.tasks]"
    pattern: "Use pytest with marker filtering to exclude slow tests: pytest test -vv -m 'not java and not rust and not erlang'"
  - source: "test/conftest.py"
    pattern: "Tests use .venv/Scripts/python.exe for Windows execution"
  - source: "Story R2 AC-R2.2"
    pattern: "JSON schema requires: story_id, phase, executed_at, test_command, summary, pass_rate, tests array, integration_checks"

# Test Requirements
test_requirements:
  unit:
    - "Verify test-results artifact file exists at .claude/sprint/test-results/6-results.json"
    - "Validate JSON format is parseable: python -c 'import json; json.load(open(\".claude/sprint/test-results/6-results.json\"))'"
    - "Verify all required schema fields present: story_id, phase, executed_at, test_command, summary, pass_rate, tests, integration_checks"
  integration:
    - "Verify test count in summary.total matches actual pytest collection count"
    - "Verify pass_rate calculation is accurate: passed / (total - skipped)"
    - "Verify tests array contains individual test entries with name, file, status, duration_ms"
    - "Verify integration_checks contains MCP and LSP validation status"
  security: []

# Acceptance Criteria Mapping
acceptance_criteria:
  - id: "AC-R2.1"
    text: "Test-results artifact exists at `.claude/sprint/test-results/6-results.json`"
    implementation: "files_to_create[0]"
    tests: "test_requirements.unit[0]"

  - id: "AC-R2.2"
    text: "JSON format matches required schema (story_id, summary, pass_rate, tests array)"
    implementation: "files_to_create[0]"
    tests: "test_requirements.unit[1,2]"

  - id: "AC-R2.3"
    text: "Test count is documented (verify if 138 is accurate)"
    implementation: "files_to_create[0]"
    tests: "test_requirements.integration[0]"

  - id: "AC-R2.4"
    text: "pass_rate field is accurate based on actual test execution"
    implementation: "files_to_create[0]"
    tests: "test_requirements.integration[1]"

  - id: "AC-R2.5"
    text: "Validation story -6.t can be unblocked after this remediation"
    implementation: "files_to_create[0]"
    tests: "test_requirements.integration[3]"

# Implementation Steps
implementation_steps:
  - step: 1
    description: "Create test-results directory"
    command: "mkdir -p .claude/sprint/test-results"

  - step: 2
    description: "Run pytest with recommended markers to exclude slow tests"
    command: ".venv/Scripts/python.exe -m pytest test/ -vv -m 'not java and not rust and not erlang' --tb=short --json-report --json-report-file=.claude/sprint/test-results/pytest-raw.json"
    fallback: ".venv/Scripts/python.exe -m pytest test/ -vv -m 'not java and not rust and not erlang' --tb=short 2>&1 | tee .claude/sprint/test-results/pytest-output.txt"
    notes: "Use json-report plugin if available, otherwise capture text output and parse manually"

  - step: 3
    description: "Parse pytest output and create JSON artifact"
    notes: |
      Parse test results from pytest output and create structured JSON with:
      - story_id: "6"
      - phase: "testing"
      - executed_at: ISO timestamp
      - test_command: actual pytest command used
      - summary: {total, passed, failed, skipped, errors}
      - pass_rate: calculated as passed / (total - skipped)
      - tests: array of individual test results
      - integration_checks: manual verification results

  - step: 4
    description: "Validate JSON artifact"
    command: "python -c \"import json; data = json.load(open('.claude/sprint/test-results/6-results.json')); assert 'story_id' in data; assert 'summary' in data; assert 'pass_rate' in data; assert 'tests' in data; print('Validation passed')\""

  - step: 5
    description: "Document test count discrepancy"
    notes: |
      Original Story 6 claimed "138 tests passed"
      Actual pytest collection: 1030 tests collected
      Discrepancy likely due to:
      - Parameterized tests (e.g., language_server fixture with multiple languages)
      - Tests added after Story 6 completion
      - Miscounting in original execution
      Document actual count in artifact and update validation story

# Discovery Findings
discovery_findings:
  test_count_discrepancy:
    claimed: 138
    actual_collected: 1030
    difference: 892
    explanation: |
      The test suite uses pytest parametrization extensively. Tests like
      test_python_basic, test_typescript_basic, etc. are parameterized across
      multiple languages, creating many test instances from single test functions.
      The original count of 138 may have been:
      - A manual count of test files/functions (not instances)
      - Run with restrictive markers that excluded most tests
      - A typo or estimation

  recommended_test_command:
    command: ".venv/Scripts/python.exe -m pytest test/ -vv -m 'not java and not rust and not erlang'"
    rationale: |
      - Excludes slow/problematic language servers (Java, Rust, Erlang)
      - Runs majority of stable tests (~800-900 tests)
      - Completes in reasonable time (5-10 minutes)
      - Follows project's poe task pattern

  integration_checks_required:
    mcp_server_starts:
      description: "Verify MCP server starts with both 'serena' and 'claude-code' context names"
      test_file: "test/serena/test_mcp.py"
      notes: "Story 2 renamed ide-assistant to claude-code, both names should work"

    lsp_backend_initializes:
      description: "Verify LSP backend initializes for supported languages"
      test_files: "test/solidlsp/*/test_*_basic.py"
      notes: "Multiple language servers should initialize without errors"

    context_names_work:
      description: "Verify both legacy and new context names are accepted"
      related_story: "Story 2 (Rename ide-assistant to claude-code)"

    path_resolution_works:
      description: "Verify centralized storage paths resolve correctly"
      related_story: "Story 5 (Merge Tool and Resource Updates)"
      test_approach: "Check SERENA_MANAGED_DIR paths in test fixtures"

# Token Budget Analysis
token_budget:
  discovery_phase: 12000
  implementation_phase: 8000
  testing_phase: 3000
  total: 23000
  notes: |
    Discovery phase is higher than typical due to:
    - Test count investigation (1030 vs 138)
    - Understanding pytest configuration
    - Analyzing parameterized test patterns
